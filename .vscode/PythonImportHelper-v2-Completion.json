[
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "sklearn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sklearn",
        "description": "sklearn",
        "detail": "sklearn",
        "documentation": {}
    },
    {
        "label": "datasets",
        "importPath": "sklearn",
        "description": "sklearn",
        "isExtraImport": true,
        "detail": "sklearn",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "OneHotEncoder",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "OneHotEncoder",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "OneHotEncoder",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "chain",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "load_training_set",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "load_test_set",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "nltk",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "nltk",
        "description": "nltk",
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "os,sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.sys",
        "description": "os.sys",
        "detail": "os.sys",
        "documentation": {}
    },
    {
        "label": "backpropagation_vectorized",
        "importPath": "propagation",
        "description": "propagation",
        "isExtraImport": true,
        "detail": "propagation",
        "documentation": {}
    },
    {
        "label": "forward_propagation",
        "importPath": "propagation",
        "description": "propagation",
        "isExtraImport": true,
        "detail": "propagation",
        "documentation": {}
    },
    {
        "label": "cost_function",
        "importPath": "propagation",
        "description": "propagation",
        "isExtraImport": true,
        "detail": "propagation",
        "documentation": {}
    },
    {
        "label": "debug_text",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "debug_text",
        "description": "debug_text",
        "detail": "debug_text",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "kind": 2,
        "importPath": "datasets.load_digits",
        "description": "datasets.load_digits",
        "peekOfCode": "def load_dataset():\n    # Load the digits dataset (0–9)\n    digits = datasets.load_digits()\n    X = pd.DataFrame(digits.data)\n    print(X)\n    y = pd.Series(digits.target)\n    # Add target to the dataframe\n    X['label'] = y\n    X = X.dropna(axis=1, how='all')                 # drop all-NaN columns\n    X = X.loc[:, ~(X == 0).all()]                  # drop all-0 columns",
        "detail": "datasets.load_digits",
        "documentation": {}
    },
    {
        "label": "stratified_k_fold_split",
        "kind": 2,
        "importPath": "ensemble_algorithm.en",
        "description": "ensemble_algorithm.en",
        "peekOfCode": "def stratified_k_fold_split(X, y, k):\n    df = pd.DataFrame(X)\n    df['label'] = y.ravel()\n    df['original_index'] = np.arange(len(df))\n    class_0 = df[df['label'] == 0].sample(frac=1).reset_index(drop=True)\n    class_1 = df[df['label'] == 1].sample(frac=1).reset_index(drop=True)\n    folds = []\n    for i in range(k):\n        c0 = class_0.iloc[int(len(class_0)*i/k):int(len(class_0)*(i+1)/k)]\n        c1 = class_1.iloc[int(len(class_1)*i/k):int(len(class_1)*(i+1)/k)]",
        "detail": "ensemble_algorithm.en",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "kind": 2,
        "importPath": "ensemble_algorithm.en",
        "description": "ensemble_algorithm.en",
        "peekOfCode": "def load_dataset(DATASET_NAME):\n    # Load dataset from CSV file\n    df = pd.read_csv(f\"../datasets/{DATASET_NAME}.csv\")\n    # === parkinsons --> customize datset ===\n    if DATASET_NAME==\"parkinsons\":\n        # change last column as label\n        df.rename(columns={\"Diagnosis\": \"label\"}, inplace=True)\n    # Use 'diagnosis' column if 'label' doesn't exist\n    if 'label' not in df.columns:\n        if 'Diagnosis' in df.columns:",
        "detail": "ensemble_algorithm.en",
        "documentation": {}
    },
    {
        "label": "normalize_train_test",
        "kind": 2,
        "importPath": "ensemble_algorithm.en",
        "description": "ensemble_algorithm.en",
        "peekOfCode": "def normalize_train_test(train_df, test_df):\n    features = train_df.drop(columns=['label', 'original_index']).columns\n    train_X = train_df[features]\n    test_X = test_df[features]\n    min_vals = train_X.min()\n    max_vals = train_X.max()\n    diff = max_vals - min_vals\n    diff[diff == 0] = 1e-8\n    train_X_norm = (train_X - min_vals) / diff\n    test_X_norm = (test_X - min_vals) / diff",
        "detail": "ensemble_algorithm.en",
        "documentation": {}
    },
    {
        "label": "majority_vote",
        "kind": 2,
        "importPath": "ensemble_algorithm.en",
        "description": "ensemble_algorithm.en",
        "peekOfCode": "def majority_vote(votes):\n    return Counter(votes).most_common(1)[0][0]\n# === KNN Fold Runner ===\ndef run_knn_single_fold(X_train, y_train, X_test, k=5):\n    def euclidean(x1, x2):\n        return np.sqrt(np.sum((x1 - x2) ** 2))\n    def majority_vote(neighbors):\n        count = Counter(neighbors)\n        return count.most_common(1)[0][0]\n    predictions = []",
        "detail": "ensemble_algorithm.en",
        "documentation": {}
    },
    {
        "label": "run_knn_single_fold",
        "kind": 2,
        "importPath": "ensemble_algorithm.en",
        "description": "ensemble_algorithm.en",
        "peekOfCode": "def run_knn_single_fold(X_train, y_train, X_test, k=5):\n    def euclidean(x1, x2):\n        return np.sqrt(np.sum((x1 - x2) ** 2))\n    def majority_vote(neighbors):\n        count = Counter(neighbors)\n        return count.most_common(1)[0][0]\n    predictions = []\n    for test_point in X_test:\n        distances = [euclidean(test_point, x_train) for x_train in X_train]\n        neighbors_idx = np.argsort(distances)[:k]",
        "detail": "ensemble_algorithm.en",
        "documentation": {}
    },
    {
        "label": "run_tree_single_fold",
        "kind": 2,
        "importPath": "ensemble_algorithm.en",
        "description": "ensemble_algorithm.en",
        "peekOfCode": "def run_tree_single_fold(X_train, y_train, X_test, ntrees=50):\n    class Node:\n        def __init__(self, feature=None, threshold=None, label=None, children=None):\n            self.feature = feature\n            self.threshold = threshold\n            self.label = label\n            self.children = children if children else {}\n    def entropy(y):\n        probabilities = y.value_counts() / len(y)\n        return -np.sum(probabilities * np.log2(probabilities))",
        "detail": "ensemble_algorithm.en",
        "documentation": {}
    },
    {
        "label": "run_nn_single_fold",
        "kind": 2,
        "importPath": "ensemble_algorithm.en",
        "description": "ensemble_algorithm.en",
        "peekOfCode": "def run_nn_single_fold(X_train, y_train, X_test):\n    class NeuralNetwork:\n        def __init__(self, layer_sizes, alpha=0.01, lambda_reg=0.0):\n            self.layer_sizes = layer_sizes\n            self.alpha = alpha\n            self.lambda_reg = lambda_reg\n            self.weights = self.initialize_weights()\n        def initialize_weights(self):\n            weights = []\n            for i in range(len(self.layer_sizes) - 1):",
        "detail": "ensemble_algorithm.en",
        "documentation": {}
    },
    {
        "label": "ensemble_main",
        "kind": 2,
        "importPath": "ensemble_algorithm.en",
        "description": "ensemble_algorithm.en",
        "peekOfCode": "def ensemble_main():\n    X, y = load_dataset(DATASET_NAME)\n    folds = stratified_k_fold_split(X, y, K_FOLD_SIZE)\n    vote_dict = defaultdict(list)\n    for fold_idx, (train_df, test_df) in enumerate(folds):\n        print(f\"\\n🔁 Fold {fold_idx+1}/{K_FOLD_SIZE}\")\n        train_df, test_df = normalize_train_test(train_df, test_df)\n        X_train = train_df.drop(columns=['label']).values\n        y_train = train_df['label'].values.ravel()\n        X_test = test_df.drop(columns=['label']).values",
        "detail": "ensemble_algorithm.en",
        "documentation": {}
    },
    {
        "label": "K_FOLD_SIZE",
        "kind": 5,
        "importPath": "ensemble_algorithm.en",
        "description": "ensemble_algorithm.en",
        "peekOfCode": "K_FOLD_SIZE = 10\n# === Stratified K-Fold Split Function ===\ndef stratified_k_fold_split(X, y, k):\n    df = pd.DataFrame(X)\n    df['label'] = y.ravel()\n    df['original_index'] = np.arange(len(df))\n    class_0 = df[df['label'] == 0].sample(frac=1).reset_index(drop=True)\n    class_1 = df[df['label'] == 1].sample(frac=1).reset_index(drop=True)\n    folds = []\n    for i in range(k):",
        "detail": "ensemble_algorithm.en",
        "documentation": {}
    },
    {
        "label": "load_data",
        "kind": 2,
        "importPath": "knn_algorithm.knn",
        "description": "knn_algorithm.knn",
        "peekOfCode": "def load_data(DATASET_NAME):\n    data_file = pd.read_csv(f'../datasets/{DATASET_NAME}.csv', header=None)\n    if DATASET_NAME == \"parkinsons\":\n        data_file.rename(columns={\"Diagnosis\": \"label\"}, inplace=True)\n    data_file = data_file.apply(pd.to_numeric, errors='coerce')\n    data_file = data_file.drop(index=0).reset_index(drop=True)\n    return data_file\n# === Stratified K-Fold Split ===\ndef stratified_k_fold_split(X, y, k):\n    df = pd.DataFrame(X)",
        "detail": "knn_algorithm.knn",
        "documentation": {}
    },
    {
        "label": "stratified_k_fold_split",
        "kind": 2,
        "importPath": "knn_algorithm.knn",
        "description": "knn_algorithm.knn",
        "peekOfCode": "def stratified_k_fold_split(X, y, k):\n    df = pd.DataFrame(X)\n    df['label'] = y\n    class_0 = df[df['label'] == 0].sample(frac=1).reset_index(drop=True)\n    class_1 = df[df['label'] == 1].sample(frac=1).reset_index(drop=True)\n    folds = []\n    for i in range(k):\n        c0 = class_0.iloc[int(len(class_0)*i/k):int(len(class_0)*(i+1)/k)]\n        c1 = class_1.iloc[int(len(class_1)*i/k):int(len(class_1)*(i+1)/k)]\n        test_df = pd.concat([c0, c1]).sample(frac=1).reset_index(drop=True)",
        "detail": "knn_algorithm.knn",
        "documentation": {}
    },
    {
        "label": "attribute_class",
        "kind": 2,
        "importPath": "knn_algorithm.knn",
        "description": "knn_algorithm.knn",
        "peekOfCode": "def attribute_class(data):\n    attribute_data = data.iloc[:, :-1]\n    class_data = data.iloc[:, -1]\n    return attribute_data, class_data\n# === Normalize data using Min-Max ===\ndef normalization_formula_train(train_data):\n    min_vals = np.min(train_data.to_numpy(), axis=0)\n    max_vals = np.max(train_data.to_numpy(), axis=0)\n    diff = max_vals - min_vals\n    if np.all(diff == 0):",
        "detail": "knn_algorithm.knn",
        "documentation": {}
    },
    {
        "label": "normalization_formula_train",
        "kind": 2,
        "importPath": "knn_algorithm.knn",
        "description": "knn_algorithm.knn",
        "peekOfCode": "def normalization_formula_train(train_data):\n    min_vals = np.min(train_data.to_numpy(), axis=0)\n    max_vals = np.max(train_data.to_numpy(), axis=0)\n    diff = max_vals - min_vals\n    if np.all(diff == 0):\n        print(\"Zero-variance columns:\", np.where(diff == 0))\n    normalized_train = (train_data - min_vals) / diff\n    return pd.DataFrame(normalized_train, columns=train_data.columns), min_vals, diff\ndef normalization_formula_test(test_data, min_vals, diff):\n    normalized_test = (test_data - min_vals) / diff",
        "detail": "knn_algorithm.knn",
        "documentation": {}
    },
    {
        "label": "normalization_formula_test",
        "kind": 2,
        "importPath": "knn_algorithm.knn",
        "description": "knn_algorithm.knn",
        "peekOfCode": "def normalization_formula_test(test_data, min_vals, diff):\n    normalized_test = (test_data - min_vals) / diff\n    return pd.DataFrame(normalized_test, columns=test_data.columns)\n# === Euclidean distance calculation ===\ndef euclidean_formula(vector1, vector2):\n    return np.sqrt(np.sum((vector1 - vector2) ** 2))\ndef euclidean_matrix(train_data, test_data, data_info):\n    euclidean_table = np.zeros((len(train_data), len(test_data)))\n    for train_idx in range(len(train_data)):\n        for test_idx in range(len(test_data)):",
        "detail": "knn_algorithm.knn",
        "documentation": {}
    },
    {
        "label": "euclidean_formula",
        "kind": 2,
        "importPath": "knn_algorithm.knn",
        "description": "knn_algorithm.knn",
        "peekOfCode": "def euclidean_formula(vector1, vector2):\n    return np.sqrt(np.sum((vector1 - vector2) ** 2))\ndef euclidean_matrix(train_data, test_data, data_info):\n    euclidean_table = np.zeros((len(train_data), len(test_data)))\n    for train_idx in range(len(train_data)):\n        for test_idx in range(len(test_data)):\n            euclidean_table[train_idx, test_idx] = euclidean_formula(train_data.iloc[train_idx], test_data.iloc[test_idx])\n    euclidean_df = pd.DataFrame(euclidean_table, index=[f\"Train_{i}\" for i in range(len(train_data))], columns=[f\"Test_{j}\" for j in range(len(test_data))])\n    print(\"--> Euclidean distance matrix has been created : \" + data_info + \"_data...\")\n    return euclidean_df",
        "detail": "knn_algorithm.knn",
        "documentation": {}
    },
    {
        "label": "euclidean_matrix",
        "kind": 2,
        "importPath": "knn_algorithm.knn",
        "description": "knn_algorithm.knn",
        "peekOfCode": "def euclidean_matrix(train_data, test_data, data_info):\n    euclidean_table = np.zeros((len(train_data), len(test_data)))\n    for train_idx in range(len(train_data)):\n        for test_idx in range(len(test_data)):\n            euclidean_table[train_idx, test_idx] = euclidean_formula(train_data.iloc[train_idx], test_data.iloc[test_idx])\n    euclidean_df = pd.DataFrame(euclidean_table, index=[f\"Train_{i}\" for i in range(len(train_data))], columns=[f\"Test_{j}\" for j in range(len(test_data))])\n    print(\"--> Euclidean distance matrix has been created : \" + data_info + \"_data...\")\n    return euclidean_df\n# === Select k-nearest neighbors ===\ndef cutoff_k(test_column, k_num):",
        "detail": "knn_algorithm.knn",
        "documentation": {}
    },
    {
        "label": "cutoff_k",
        "kind": 2,
        "importPath": "knn_algorithm.knn",
        "description": "knn_algorithm.knn",
        "peekOfCode": "def cutoff_k(test_column, k_num):\n    smallest_column = test_column.sort_values(ascending=True)[:k_num]\n    smallest_indices = smallest_column.index.str.split('_').str[1].astype(int)\n    return smallest_indices\n# === Determine majority label ===\ndef majority_formula(list):\n    count_1 = list.value_counts().get(1, 0)\n    count_0 = list.value_counts().get(0, 0)\n    return 1 if count_1 > count_0 else 0\n# === Accuracy calculation ===",
        "detail": "knn_algorithm.knn",
        "documentation": {}
    },
    {
        "label": "majority_formula",
        "kind": 2,
        "importPath": "knn_algorithm.knn",
        "description": "knn_algorithm.knn",
        "peekOfCode": "def majority_formula(list):\n    count_1 = list.value_counts().get(1, 0)\n    count_0 = list.value_counts().get(0, 0)\n    return 1 if count_1 > count_0 else 0\n# === Accuracy calculation ===\ndef calculate_accuracy(actual_series, predicted_series):\n    actual_list = np.array(actual_series.tolist())\n    predicted_list = np.array(predicted_series, dtype=int)\n    match_count = np.sum(actual_list == predicted_list)\n    return match_count / len(actual_list)",
        "detail": "knn_algorithm.knn",
        "documentation": {}
    },
    {
        "label": "calculate_accuracy",
        "kind": 2,
        "importPath": "knn_algorithm.knn",
        "description": "knn_algorithm.knn",
        "peekOfCode": "def calculate_accuracy(actual_series, predicted_series):\n    actual_list = np.array(actual_series.tolist())\n    predicted_list = np.array(predicted_series, dtype=int)\n    match_count = np.sum(actual_list == predicted_list)\n    return match_count / len(actual_list)\n# === F1 Score calculation ===\ndef calculate_f1score(actual_series, predicted_series):\n    actual = np.array(actual_series.tolist(), dtype=int)\n    predicted = np.array(predicted_series.tolist(), dtype=int)\n    TP = np.sum((actual == 1) & (predicted == 1))",
        "detail": "knn_algorithm.knn",
        "documentation": {}
    },
    {
        "label": "calculate_f1score",
        "kind": 2,
        "importPath": "knn_algorithm.knn",
        "description": "knn_algorithm.knn",
        "peekOfCode": "def calculate_f1score(actual_series, predicted_series):\n    actual = np.array(actual_series.tolist(), dtype=int)\n    predicted = np.array(predicted_series.tolist(), dtype=int)\n    TP = np.sum((actual == 1) & (predicted == 1))\n    FP = np.sum((actual == 0) & (predicted == 1))\n    FN = np.sum((actual == 1) & (predicted == 0))\n    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n    return 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0\n# === Run KNN for given k values ===",
        "detail": "knn_algorithm.knn",
        "documentation": {}
    },
    {
        "label": "knn_algorithm",
        "kind": 2,
        "importPath": "knn_algorithm.knn",
        "description": "knn_algorithm.knn",
        "peekOfCode": "def knn_algorithm(k, test_euclidean, predicted_class, actual_class, data_info, fold_count, accuracy_f1_table):\n    predicted_table = pd.DataFrame()\n    for k_num in range(1, k+1, 2):\n        for test_num in range(len(test_euclidean.columns)):\n            cutoff_indices = cutoff_k(test_euclidean[f\"Test_{test_num}\"], k_num)\n            predicted_list = predicted_class.iloc[cutoff_indices]\n            predicted_value = majority_formula(predicted_list)\n            predicted_table.at[f\"Test_{test_num}\", f\"k={k_num}\"] = int(predicted_value)\n            print(f\"knn algorithm : test_data={data_info} , fold={fold_count} , k={k_num} , test_instance={test_num}\")\n        accuracy_value = calculate_accuracy(actual_class, predicted_table[f\"k={k_num}\"])",
        "detail": "knn_algorithm.knn",
        "documentation": {}
    },
    {
        "label": "accuracy_avg_std",
        "kind": 2,
        "importPath": "knn_algorithm.knn",
        "description": "knn_algorithm.knn",
        "peekOfCode": "def accuracy_avg_std(accuracy_f1_table, data_info):\n    meanstd = accuracy_f1_table.agg(['mean', 'std'])\n    graph_table = pd.concat([accuracy_f1_table, meanstd])\n    print(\"\\n--> Calculate mean and standard deviation of each k value : \" + str(data_info) + \"...\")\n    return graph_table\n# === Draw accuracy graph with error bars ===\ndef draw_graph(accuracy_f1_table, title):\n    accuracy_table = accuracy_f1_table[[col for col in accuracy_f1_table.columns if \"accuracy\" in col]]\n    k_values = [int(re.search(r'\\(k=(\\d+)\\)', col).group(1)) for col in accuracy_table.columns]\n    mean_values = accuracy_table.loc['mean'].tolist()",
        "detail": "knn_algorithm.knn",
        "documentation": {}
    },
    {
        "label": "draw_graph",
        "kind": 2,
        "importPath": "knn_algorithm.knn",
        "description": "knn_algorithm.knn",
        "peekOfCode": "def draw_graph(accuracy_f1_table, title):\n    accuracy_table = accuracy_f1_table[[col for col in accuracy_f1_table.columns if \"accuracy\" in col]]\n    k_values = [int(re.search(r'\\(k=(\\d+)\\)', col).group(1)) for col in accuracy_table.columns]\n    mean_values = accuracy_table.loc['mean'].tolist()\n    std_values = accuracy_table.loc['std'].tolist()\n    plt.figure(figsize=(6, 4))\n    plt.errorbar(k_values, mean_values, yerr=std_values, fmt='o-', capsize=5)\n    plt.xlabel(\"(Value of k)\")\n    plt.ylabel(\"(Accuracy over \" + title + \" data\")\n    plt.savefig(f'evaluation/{DATASET_NAME}_' + title + \".png\", dpi=300, bbox_inches='tight')",
        "detail": "knn_algorithm.knn",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "knn_algorithm.knn",
        "description": "knn_algorithm.knn",
        "peekOfCode": "def main(DATASET_NAME):\n    # if the folder is not existed, create one\n    os.makedirs(\"evaluation\", exist_ok=True) \n    os.makedirs(\"normalization\", exist_ok=True) \n    data_file = load_data(DATASET_NAME)\n    attributes, labels = attribute_class(data_file)\n    folds = stratified_k_fold_split(attributes, labels, K_FOLD_SIZE)\n    train_accuracy = pd.DataFrame()\n    test_accuracy = pd.DataFrame()\n    for fold_count, (train_df, test_df) in enumerate(folds, start=1):",
        "detail": "knn_algorithm.knn",
        "documentation": {}
    },
    {
        "label": "LogisticRegression",
        "kind": 6,
        "importPath": "logistic_regression.reg",
        "description": "logistic_regression.reg",
        "peekOfCode": "class LogisticRegression:\n    def __init__(self, learning_rate=0.01, epochs=1000):\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        self.w = np.zeros((n_features, 1))  # initialize weights\n        self.b = 0.0                        # initialize bias\n        for _ in range(self.epochs):\n            linear_model = np.dot(X, self.w) + self.b",
        "detail": "logistic_regression.reg",
        "documentation": {}
    },
    {
        "label": "my_accuracy",
        "kind": 2,
        "importPath": "logistic_regression.reg",
        "description": "logistic_regression.reg",
        "peekOfCode": "def my_accuracy(y_true, y_pred):\n    correct = np.sum(y_true == y_pred)\n    return correct / len(y_true)\n# === F1 Score Calculation ===\ndef my_f1_score(y_true, y_pred):\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    if tp == 0:\n        return 0.0",
        "detail": "logistic_regression.reg",
        "documentation": {}
    },
    {
        "label": "my_f1_score",
        "kind": 2,
        "importPath": "logistic_regression.reg",
        "description": "logistic_regression.reg",
        "peekOfCode": "def my_f1_score(y_true, y_pred):\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    if tp == 0:\n        return 0.0\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n    if precision + recall == 0:\n        return 0.0",
        "detail": "logistic_regression.reg",
        "documentation": {}
    },
    {
        "label": "sigmoid",
        "kind": 2,
        "importPath": "logistic_regression.reg",
        "description": "logistic_regression.reg",
        "peekOfCode": "def sigmoid(z):\n    z = np.clip(z, -500, 500)  # prevent overflow in np.exp()\n    return 1 / (1 + np.exp(-z))\n# === Logistic Regression from Scratch ===\nclass LogisticRegression:\n    def __init__(self, learning_rate=0.01, epochs=1000):\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n    def fit(self, X, y):\n        n_samples, n_features = X.shape",
        "detail": "logistic_regression.reg",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "kind": 2,
        "importPath": "logistic_regression.reg",
        "description": "logistic_regression.reg",
        "peekOfCode": "def load_dataset(DATASET_NAME):\n    df = pd.read_csv(f\"../datasets/{DATASET_NAME}.csv\")\n    if DATASET_NAME == \"parkinsons\":\n        df.rename(columns={\"Diagnosis\": \"label\"}, inplace=True)\n    if 'label' not in df.columns:\n        if 'Diagnosis' in df.columns:\n            df = df.rename(columns={'Diagnosis': 'label'})\n            print(\"🛈 Renamed 'Diagnosis' to 'label' for compatibility.\")\n        else:\n            raise ValueError(\"Dataset must contain a 'label' or 'diagnosis' column.\")",
        "detail": "logistic_regression.reg",
        "documentation": {}
    },
    {
        "label": "stratified_k_fold_split",
        "kind": 2,
        "importPath": "logistic_regression.reg",
        "description": "logistic_regression.reg",
        "peekOfCode": "def stratified_k_fold_split(X, y, k):\n    df = pd.DataFrame(X)\n    df['label'] = y.ravel()\n    class_0 = df[df['label'] == 0].sample(frac=1).reset_index(drop=True)\n    class_1 = df[df['label'] == 1].sample(frac=1).reset_index(drop=True)\n    folds = []\n    for i in range(k):\n        c0 = class_0.iloc[int(len(class_0)*i/k):int(len(class_0)*(i+1)/k)]\n        c1 = class_1.iloc[int(len(class_1)*i/k):int(len(class_1)*(i+1)/k)]\n        test_df = pd.concat([c0, c1]).sample(frac=1).reset_index(drop=True)",
        "detail": "logistic_regression.reg",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "logistic_regression.reg",
        "description": "logistic_regression.reg",
        "peekOfCode": "def evaluate(DATASET_NAME, k=5):\n    X, y = load_dataset(DATASET_NAME)\n    folds = stratified_k_fold_split(X, y, k=K_FOLD_SIZE)\n    acc_list = []\n    f1_list = []\n    for i, (train_df, test_df) in enumerate(folds):\n        X_train = train_df.drop(columns=['label']).values\n        y_train = train_df['label'].values.reshape(-1, 1)\n        X_test = test_df.drop(columns=['label']).values\n        y_test = test_df['label'].values.reshape(-1, 1)",
        "detail": "logistic_regression.reg",
        "documentation": {}
    },
    {
        "label": "log_func",
        "kind": 2,
        "importPath": "naive_bayes.nb",
        "description": "naive_bayes.nb",
        "peekOfCode": "def log_func(num):\n    return np.log10(num)\n######## likelihood_common ########       \n# for train dataset\ndef word_count_set(data_set):\n    # single test document => just list\n    all_words = [word for sublist in data_set for word in sublist]\n    word_counts = Counter(all_words)\n    # each count\n    freq_dataset=pd.DataFrame(word_counts.items(),columns=[\"word\",\"freq_train\"])",
        "detail": "naive_bayes.nb",
        "documentation": {}
    },
    {
        "label": "word_count_set",
        "kind": 2,
        "importPath": "naive_bayes.nb",
        "description": "naive_bayes.nb",
        "peekOfCode": "def word_count_set(data_set):\n    # single test document => just list\n    all_words = [word for sublist in data_set for word in sublist]\n    word_counts = Counter(all_words)\n    # each count\n    freq_dataset=pd.DataFrame(word_counts.items(),columns=[\"word\",\"freq_train\"])\n    freq_dataset.set_index([\"word\"], drop=True, inplace=True)\n    return freq_dataset\n######## likelihood_standard ######## \n# for test dataset",
        "detail": "naive_bayes.nb",
        "documentation": {}
    },
    {
        "label": "word_count_list",
        "kind": 2,
        "importPath": "naive_bayes.nb",
        "description": "naive_bayes.nb",
        "peekOfCode": "def word_count_list(data_list):\n    # single test document => just list\n    all_words = [word for word in data_list]\n    word_counts = Counter(all_words)\n    # each count\n    freq_dataset=pd.DataFrame(word_counts.items(),columns=[\"word\",\"freq_test\"])\n    freq_dataset.set_index([\"word\"], drop=True, inplace=True)\n    return freq_dataset\n######## posterior_standard, posterior_laplace ########     \n# Compare probability",
        "detail": "naive_bayes.nb",
        "documentation": {}
    },
    {
        "label": "compare_probability",
        "kind": 2,
        "importPath": "naive_bayes.nb",
        "description": "naive_bayes.nb",
        "peekOfCode": "def compare_probability(posterior_pos, posterior_neg):\n    if posterior_pos > posterior_neg:\n        result = \"Positive\"\n        print(result)\n    elif posterior_pos < posterior_neg:\n        result = \"Negative\"\n        print(result)\n    elif posterior_pos == posterior_neg:\n        print(\"RANDOM~~~\")\n        result = random.choice([\"Positive\", \"Negative\"])",
        "detail": "naive_bayes.nb",
        "documentation": {}
    },
    {
        "label": "multiply_standard",
        "kind": 2,
        "importPath": "naive_bayes.nb",
        "description": "naive_bayes.nb",
        "peekOfCode": "def multiply_standard(likelihood_sorted):  \n    # copy the original version\n    likelihood_sorted = likelihood_sorted.copy()\n    # before multiply, make portion : NaN => 0\n    likelihood_sorted.loc[:, \"portion_train_pos\"] = likelihood_sorted[\"portion_train_pos\"].fillna(0)\n    likelihood_sorted.loc[:, \"portion_train_neg\"] = likelihood_sorted[\"portion_train_neg\"].fillna(0)\n    # (portion_train_pos) ^ (freq_test)\n    likelihood_sorted.loc[:, \"multiply_pos\"] = likelihood_sorted[\"portion_train_pos\"] ** likelihood_sorted[\"freq_test\"]\n    likelihood_sorted.loc[:, \"multiply_neg\"] = likelihood_sorted[\"portion_train_neg\"] ** likelihood_sorted[\"freq_test\"]\n    # likely_pos, likely_neg = multiply all the result",
        "detail": "naive_bayes.nb",
        "documentation": {}
    },
    {
        "label": "sum_laplace",
        "kind": 2,
        "importPath": "naive_bayes.nb",
        "description": "naive_bayes.nb",
        "peekOfCode": "def sum_laplace(likelihood_alpha):  \n    # copy the original version\n    likelihood_alpha = likelihood_alpha.copy()\n    # (portion_train_pos) * log(freq_test)\n    likelihood_alpha.loc[:, \"sum_pos\"] = likelihood_alpha[\"freq_test\"] * log_func(likelihood_alpha[\"portion_train_pos_alpha\"])\n    likelihood_alpha.loc[:, \"sum_neg\"] = likelihood_alpha[\"freq_test\"] * log_func(likelihood_alpha[\"portion_train_neg_alpha\"])\n    # likely_pos, likely_neg = multiply all the result\n    likely_pos=likelihood_alpha[\"sum_pos\"].sum()\n    likely_neg=likelihood_alpha[\"sum_neg\"].sum()\n    return likely_pos, likely_neg",
        "detail": "naive_bayes.nb",
        "documentation": {}
    },
    {
        "label": "draw_table",
        "kind": 2,
        "importPath": "naive_bayes.nb",
        "description": "naive_bayes.nb",
        "peekOfCode": "def draw_table(confusion_table, accuracy, precision, recall, image_name):\n    fig, ax = plt.subplots(figsize=(6.5, 2.2))  # Create figure and axes\n    ax.axis('off')  # Hide axes\n    # Display the table\n    table = ax.table(cellText=confusion_table.values, colLabels=confusion_table.columns, rowLabels=confusion_table.index, cellLoc='center',  loc='center')\n    table.auto_set_column_width(13)\n    # Adjust table scale for better visibility\n    table.scale(1.5, 1.5)\n    # Set font size for table\n    table.auto_set_font_size(False)  ",
        "detail": "naive_bayes.nb",
        "documentation": {}
    },
    {
        "label": "draw_graph",
        "kind": 2,
        "importPath": "naive_bayes.nb",
        "description": "naive_bayes.nb",
        "peekOfCode": "def draw_graph(alpha_list, accuracy_list):\n    log_alpha_list = log_func(alpha_list) \n    print(log_alpha_list)\n    plt.figure(figsize=(8, 6))\n    plt.plot(log_alpha_list, accuracy_list, marker='o', linestyle='-')\n    plt.xlabel('alpha (log scale)')\n    plt.ylabel('accuracy')\n    plt.title(\"Figure 2-Graph. Plot of the model's accuracy\")\n    plt.legend()\n    plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)",
        "detail": "naive_bayes.nb",
        "documentation": {}
    },
    {
        "label": "preprocessing_data",
        "kind": 2,
        "importPath": "naive_bayes.nb",
        "description": "naive_bayes.nb",
        "peekOfCode": "def preprocessing_data(train_percen_pos, train_percen_neg, test_percen_pos, test_percen_neg):\n    percentage_positive_instances_train = train_percen_pos\n    percentage_negative_instances_train = train_percen_neg\n    percentage_positive_instances_test = test_percen_pos\n    percentage_negative_instances_test = test_percen_neg\n    (train_pos, train_neg, vocab) = load_training_set(percentage_positive_instances_train, percentage_negative_instances_train)\n    (test_pos, test_neg) = load_test_set(percentage_positive_instances_test, percentage_negative_instances_test)\n    print(\"\\nTrain and Test dataset are completed to preprocessing data\")\n    return train_pos, train_neg, vocab, test_pos, test_neg\n##################################################################",
        "detail": "naive_bayes.nb",
        "documentation": {}
    },
    {
        "label": "prior_standard",
        "kind": 2,
        "importPath": "naive_bayes.nb",
        "description": "naive_bayes.nb",
        "peekOfCode": "def prior_standard(pos_data_len, neg_data_len):\n    total_len = pos_data_len + neg_data_len\n    return pos_data_len / total_len, neg_data_len / total_len\n# Laplace : Q2 ~ Q6\ndef prior_laplace(pos_data_len, neg_data_len):\n    pos_prior, neg_prior = prior_standard(pos_data_len, neg_data_len)\n    return log_func(pos_prior), log_func(neg_prior)\n##################################################################\n########### 3. Likelihood ###########\n# likelihood_common => contain test and train data freq_pos, freq_neg",
        "detail": "naive_bayes.nb",
        "documentation": {}
    },
    {
        "label": "prior_laplace",
        "kind": 2,
        "importPath": "naive_bayes.nb",
        "description": "naive_bayes.nb",
        "peekOfCode": "def prior_laplace(pos_data_len, neg_data_len):\n    pos_prior, neg_prior = prior_standard(pos_data_len, neg_data_len)\n    return log_func(pos_prior), log_func(neg_prior)\n##################################################################\n########### 3. Likelihood ###########\n# likelihood_common => contain test and train data freq_pos, freq_neg\ndef likelihood_common(train_pos, train_neg):\n    ###### train ######\n    # input the frequecy of train data\n    freq_train_pos=word_count_set(train_pos)",
        "detail": "naive_bayes.nb",
        "documentation": {}
    },
    {
        "label": "likelihood_common",
        "kind": 2,
        "importPath": "naive_bayes.nb",
        "description": "naive_bayes.nb",
        "peekOfCode": "def likelihood_common(train_pos, train_neg):\n    ###### train ######\n    # input the frequecy of train data\n    freq_train_pos=word_count_set(train_pos)\n    freq_train_neg=word_count_set(train_neg)\n    # merge \n    likelihood_table = pd.concat([freq_train_pos, freq_train_neg],axis=1)\n    likelihood_table.columns=[\"freq_train_pos\",\"freq_train_neg\"]\n    # get the total frequency in the table\n    # use only train data -> Total_Frequency, so it will not affect to out of vocabulary, since vocab consist of train ",
        "detail": "naive_bayes.nb",
        "documentation": {}
    },
    {
        "label": "likelihood_standard",
        "kind": 2,
        "importPath": "naive_bayes.nb",
        "description": "naive_bayes.nb",
        "peekOfCode": "def likelihood_standard(likelihood_table, test_data, vocab):\n    # Calculate portions\n    pos_sum = likelihood_table.at[\"Total\", \"freq_train_pos\"] # positive -> V\n    neg_sum = likelihood_table.at[\"Total\", \"freq_train_neg\"] # negative -> V\n    likelihood_table = likelihood_table.copy()  # avoid error that return myself\n    likelihood_table[\"portion_train_pos\"] = likelihood_table[\"freq_train_pos\"] / pos_sum\n    likelihood_table[\"portion_train_neg\"] = likelihood_table[\"freq_train_neg\"] / neg_sum\n    # input the frequecy of test data\n    freq_test=word_count_list(test_data)\n    # merge ",
        "detail": "naive_bayes.nb",
        "documentation": {}
    },
    {
        "label": "likelihood_laplace",
        "kind": 2,
        "importPath": "naive_bayes.nb",
        "description": "naive_bayes.nb",
        "peekOfCode": "def likelihood_laplace(likelihood_table, test_data, alpha, vocab):\n    # Calculate portions\n    pos_sum = likelihood_table.at[\"Total\", \"freq_train_pos\"] \n    neg_sum = likelihood_table.at[\"Total\", \"freq_train_neg\"]\n    # input the frequecy of test data\n    freq_test=word_count_list(test_data)\n    # merge \n    likelihood_test = pd.concat([freq_test, likelihood_table], axis=1)\n    # sort not \"nan\" values from \"freq_test\"\n    likelihood_test = likelihood_test[likelihood_test['freq_test'].notna()].sort_values(by='freq_test')",
        "detail": "naive_bayes.nb",
        "documentation": {}
    },
    {
        "label": "posterior_standard",
        "kind": 2,
        "importPath": "naive_bayes.nb",
        "description": "naive_bayes.nb",
        "peekOfCode": "def posterior_standard(prior_pos, prior_neg, likely_pos, likely_neg):\n    # test_pos \n    posterior_pos = prior_pos * likely_pos\n    posterior_neg = prior_neg * likely_neg\n    print(f\"prior * likely | pos: {posterior_pos} / neg: {posterior_neg}\")\n    posterior_result=compare_probability(posterior_pos, posterior_neg)\n    print(f\"posterior_result : {posterior_result}\")\n    return posterior_result\n# Laplace Smoothing log(prior probability) + sum (log(likelihoodal))\ndef posterior_laplace(likelihood_table, test_data, vocab, alpha):",
        "detail": "naive_bayes.nb",
        "documentation": {}
    },
    {
        "label": "posterior_laplace",
        "kind": 2,
        "importPath": "naive_bayes.nb",
        "description": "naive_bayes.nb",
        "peekOfCode": "def posterior_laplace(likelihood_table, test_data, vocab, alpha):\n    # test_pos \n    posterior_pos = prior_pos + likely_pos\n    posterior_neg = prior_neg + likely_neg\n    print(f\"prior * likely | pos: {posterior_pos} / neg: {posterior_neg}\")\n    posterior_result=compare_probability(posterior_pos, posterior_neg)\n    print(f\"posterior_result : {posterior_result}\")\n    return posterior_result\n##################################################################\n########### 5. Confusion Metrix ###########",
        "detail": "naive_bayes.nb",
        "documentation": {}
    },
    {
        "label": "confusion_matrix",
        "kind": 2,
        "importPath": "naive_bayes.nb",
        "description": "naive_bayes.nb",
        "peekOfCode": "def confusion_matrix(posterior_pos, posterior_neg, file_name):\n    print(\"\\nMaking confusion matrix to get accuracy, precision, and recall\")\n    TP=posterior_pos.count(\"Positive\")\n    TN=posterior_neg.count(\"Negative\")\n    FP=posterior_neg.count(\"Positive\")\n    FN=posterior_pos.count(\"Negative\")\n    print(f\"TP:{TP} / FN:{FN} / TN:{TN} / FP:{FP}\")\n    # calcualte accuracy\n    accuracy=(TP+TN)/(TP+TN+FP+FN)\n    # calcualte precision",
        "detail": "naive_bayes.nb",
        "documentation": {}
    },
    {
        "label": "preprocess_text",
        "kind": 2,
        "importPath": "naive_bayes.utils",
        "description": "naive_bayes.utils",
        "peekOfCode": "def preprocess_text(text):\n    stop_words = set(stopwords.words('english'))\n    text = REPLACE_NO_SPACE.sub(\"\", text)\n    text = REPLACE_WITH_SPACE.sub(\" \", text)\n    text = re.sub(r'\\d+', '', text)\n    text = text.lower()\n    words = text.split()\n    return [w for w in words if w not in stop_words]\ndef load_training_set(percentage_positives, percentage_negatives):\n    vocab = set()",
        "detail": "naive_bayes.utils",
        "documentation": {}
    },
    {
        "label": "load_training_set",
        "kind": 2,
        "importPath": "naive_bayes.utils",
        "description": "naive_bayes.utils",
        "peekOfCode": "def load_training_set(percentage_positives, percentage_negatives):\n    vocab = set()\n    positive_instances = []\n    negative_instances = []\n    df = pd.read_csv('train-positive.csv')\n    for _, contents in df.iterrows():\n        contents = contents['reviewText']\n        if random.random() > percentage_positives:\n            continue\n        contents = preprocess_text(contents)",
        "detail": "naive_bayes.utils",
        "documentation": {}
    },
    {
        "label": "load_test_set",
        "kind": 2,
        "importPath": "naive_bayes.utils",
        "description": "naive_bayes.utils",
        "peekOfCode": "def load_test_set(percentage_positives, percentage_negatives):\n    positive_instances = []\n    negative_instances = []\n    df = pd.read_csv('test-positive.csv')\n    for _, contents in df.iterrows():\n        contents = contents['reviewText']\n        if random.random() > percentage_positives:\n            continue\n        contents = preprocess_text(contents)\n        positive_instances.append(contents)",
        "detail": "naive_bayes.utils",
        "documentation": {}
    },
    {
        "label": "REPLACE_NO_SPACE",
        "kind": 5,
        "importPath": "naive_bayes.utils",
        "description": "naive_bayes.utils",
        "peekOfCode": "REPLACE_NO_SPACE = re.compile(\"[._;:!*`짝\\'?,\\\"()\\[\\]]\")\nREPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n# remove this part for mac -> add for window\nnltk.download('stopwords')\ndef preprocess_text(text):\n    stop_words = set(stopwords.words('english'))\n    text = REPLACE_NO_SPACE.sub(\"\", text)\n    text = REPLACE_WITH_SPACE.sub(\" \", text)\n    text = re.sub(r'\\d+', '', text)\n    text = text.lower()",
        "detail": "naive_bayes.utils",
        "documentation": {}
    },
    {
        "label": "REPLACE_WITH_SPACE",
        "kind": 5,
        "importPath": "naive_bayes.utils",
        "description": "naive_bayes.utils",
        "peekOfCode": "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n# remove this part for mac -> add for window\nnltk.download('stopwords')\ndef preprocess_text(text):\n    stop_words = set(stopwords.words('english'))\n    text = REPLACE_NO_SPACE.sub(\"\", text)\n    text = REPLACE_WITH_SPACE.sub(\" \", text)\n    text = re.sub(r'\\d+', '', text)\n    text = text.lower()\n    words = text.split()",
        "detail": "naive_bayes.utils",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "neural_network.debug_text",
        "description": "neural_network.debug_text",
        "peekOfCode": "def main(lambda_reg, X, y, Theta, all_a_lists, all_z_lists, J_list, final_cost, delta_list, D_list, finalized_D, DEBUG_FILENAME, header=\"\"): \n    # if the folder is not existed, create\n    os.makedirs(\"debug\", exist_ok=True) \n    # Open a debug file for writing results\n    with open(f\"debug/backprop_{DEBUG_FILENAME}.txt\", \"w\", encoding=\"utf-8\") as f:\n        if header:\n            # Write optional header if provided\n            f.write(\"=\" * 80 + \"\\n\")\n            f.write(header + \"\\n\")\n            f.write(\"=\" * 80 + \"\\n\")",
        "detail": "neural_network.debug_text",
        "documentation": {}
    },
    {
        "label": "NeuralNetwork",
        "kind": 6,
        "importPath": "neural_network.nn",
        "description": "neural_network.nn",
        "peekOfCode": "class NeuralNetwork:\n    def __init__(self, layer_sizes, alpha=0.01, lambda_reg=0.0):\n        self.layer_sizes = layer_sizes  # Architecture of the network\n        self.alpha = alpha              # Learning rate\n        self.lambda_reg = lambda_reg    # Regularization parameter\n        self.weights = self.initialize_weights()  # Random weight initialization\n        self.cost_history = []          # Store J value per training set\n    def initialize_weights(self):\n        # Initialize weights for each layer using uniform distribution\n        weights = []",
        "detail": "neural_network.nn",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "kind": 2,
        "importPath": "neural_network.nn",
        "description": "neural_network.nn",
        "peekOfCode": "def load_dataset(DATASET_NAME):\n    # Load dataset from CSV file\n    df = pd.read_csv(f\"../datasets/{DATASET_NAME}.csv\")\n    # === parkinsons --> customize datset ===\n    if DATASET_NAME==\"parkinsons\":\n        # change last column as label\n        df.rename(columns={\"Diagnosis\": \"label\"}, inplace=True)\n    # Use 'diagnosis' column if 'label' doesn't exist\n    if 'label' not in df.columns:\n        if 'Diagnosis' in df.columns:",
        "detail": "neural_network.nn",
        "documentation": {}
    },
    {
        "label": "stratified_k_fold_split",
        "kind": 2,
        "importPath": "neural_network.nn",
        "description": "neural_network.nn",
        "peekOfCode": "def stratified_k_fold_split(X, y, k):\n    # Create stratified folds with equal class distribution\n    df = pd.DataFrame(X)\n    df['label'] = y.ravel()\n    class_0 = df[df['label'] == 0].sample(frac=1).reset_index(drop=True)\n    class_1 = df[df['label'] == 1].sample(frac=1).reset_index(drop=True)\n    folds = []\n    for i in range(k):\n        c0 = class_0.iloc[int(len(class_0)*i/k):int(len(class_0)*(i+1)/k)]\n        c1 = class_1.iloc[int(len(class_1)*i/k):int(len(class_1)*(i+1)/k)]",
        "detail": "neural_network.nn",
        "documentation": {}
    },
    {
        "label": "my_accuracy",
        "kind": 2,
        "importPath": "neural_network.nn",
        "description": "neural_network.nn",
        "peekOfCode": "def my_accuracy(y_true, y_pred):\n    correct = np.sum(y_true == y_pred)\n    return correct / len(y_true)\n# === F1 Score Calculation ===\ndef my_f1_score(y_true, y_pred):\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    if tp == 0:\n        return 0.0",
        "detail": "neural_network.nn",
        "documentation": {}
    },
    {
        "label": "my_f1_score",
        "kind": 2,
        "importPath": "neural_network.nn",
        "description": "neural_network.nn",
        "peekOfCode": "def my_f1_score(y_true, y_pred):\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    if tp == 0:\n        return 0.0\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n    if precision + recall == 0:\n        return 0.0",
        "detail": "neural_network.nn",
        "documentation": {}
    },
    {
        "label": "info_text",
        "kind": 2,
        "importPath": "neural_network.nn",
        "description": "neural_network.nn",
        "peekOfCode": "def info_text(lambda_reg,hidden_layer,alpha,mode, batch_size, DATASET_NAME):\n    info = f\"{DATASET_NAME.capitalize()} BEST Learning Curve\\n λ={lambda_reg},  Hidden={hidden_layer}, \\nα={alpha}, Mode={mode}\"\n    if mode == \"mini-batch\":\n        info += f\", Batch Size={batch_size}\\n\"\n    else:\n        info += f\"\\n\"\n    if STOP_CRITERIA==\"M\":\n        info += f\"Stopping Criteria=m size [{M_SIZE}]\"   \n    elif STOP_CRITERIA==\"J\":\n        info += f\"Stopping Criteria=Final Cost(J)[{J_SIZE}]\"   ",
        "detail": "neural_network.nn",
        "documentation": {}
    },
    {
        "label": "plot_best_learning_curve",
        "kind": 2,
        "importPath": "neural_network.nn",
        "description": "neural_network.nn",
        "peekOfCode": "def plot_best_learning_curve(results, save_folder, DATASET_NAME):\n    # Identify model with lowest final cost\n    best_key = min(results, key=lambda k: results[k]['model'].cost_history[-1])\n    best_info = results[best_key]\n    model = best_info['model']\n    hidden_layer = best_info['hidden']\n    lambda_reg = best_info['lambda_reg']\n    train_size = best_info['train_size']\n    alpha = best_info['alpha']\n    mode = best_info['mode']",
        "detail": "neural_network.nn",
        "documentation": {}
    },
    {
        "label": "save_metrics_table",
        "kind": 2,
        "importPath": "neural_network.nn",
        "description": "neural_network.nn",
        "peekOfCode": "def save_metrics_table(results, save_folder, DATASET_NAME):\n    os.makedirs(\"evaluation\", exist_ok=True)\n    fig, ax = plt.subplots(figsize=(10, 8))\n    ax.axis('off')\n    # Determine table columns based on whether mini-batch was used\n    if any(val['mode'] == 'mini-batch' for val in results.values()):\n        col_labels = [\"Layer & Neuron\", \"Lambda\", \"Alpha\", \"Batch Size\", \"Mode\", \"Avg Accuracy\", \"Avg F1 Score\"]\n        show_batch_size = True\n    else:\n        col_labels = [\"Layer & Neuron\", \"Lambda\", \"Alpha\", \"Mode\", \"Avg Accuracy\", \"Avg F1 Score\"]",
        "detail": "neural_network.nn",
        "documentation": {}
    },
    {
        "label": "neural_network",
        "kind": 2,
        "importPath": "neural_network.nn",
        "description": "neural_network.nn",
        "peekOfCode": "def neural_network(DATASET_NAME):\n    # if the folder is not existed, create\n    os.makedirs(\"evaluation\", exist_ok=True) \n    X, y = load_dataset(DATASET_NAME)  # Load data and labels\n    folds = stratified_k_fold_split(X, y, k=K_FOLD_SIZE)  # Create 10-fold split\n    lambda_reg_list = LAMBDA_REG  # List of λ values to test\n    hidden_layers = HIDDEN_LAYER  # Layer architectures to test\n    alpha = ALPHA            # Learning rate\n    batch_size = BATCH_SIZE        # Size of mini-batches\n    mode = TRAIN_MODE        # Training mode",
        "detail": "neural_network.nn",
        "documentation": {}
    },
    {
        "label": "DATASET_NAME",
        "kind": 5,
        "importPath": "neural_network.nn",
        "description": "neural_network.nn",
        "peekOfCode": "DATASET_NAME = \"credit_approval\"  # Name of the dataset\nK_FOLD_SIZE= 10\nDEBUG_MODE = True         # If True, run debugging routine at the end\nTRAIN_MODE = \"mini-batch\"    # Choose \"batch\" or \"mini-batch\"\nBATCH_SIZE = 64\nALPHA=0.1\n# === Stopping Criteria ===\nSTOP_CRITERIA = \"M\"\nM_SIZE = 5000            \nJ_SIZE=0.1",
        "detail": "neural_network.nn",
        "documentation": {}
    },
    {
        "label": "DEBUG_MODE",
        "kind": 5,
        "importPath": "neural_network.nn",
        "description": "neural_network.nn",
        "peekOfCode": "DEBUG_MODE = True         # If True, run debugging routine at the end\nTRAIN_MODE = \"mini-batch\"    # Choose \"batch\" or \"mini-batch\"\nBATCH_SIZE = 64\nALPHA=0.1\n# === Stopping Criteria ===\nSTOP_CRITERIA = \"M\"\nM_SIZE = 5000            \nJ_SIZE=0.1\n# === Hyper Parameter ===\n# LAMBDA_REG=[0.000001]",
        "detail": "neural_network.nn",
        "documentation": {}
    },
    {
        "label": "TRAIN_MODE",
        "kind": 5,
        "importPath": "neural_network.nn",
        "description": "neural_network.nn",
        "peekOfCode": "TRAIN_MODE = \"mini-batch\"    # Choose \"batch\" or \"mini-batch\"\nBATCH_SIZE = 64\nALPHA=0.1\n# === Stopping Criteria ===\nSTOP_CRITERIA = \"M\"\nM_SIZE = 5000            \nJ_SIZE=0.1\n# === Hyper Parameter ===\n# LAMBDA_REG=[0.000001]\n# HIDDEN_LAYER=[[64]]",
        "detail": "neural_network.nn",
        "documentation": {}
    },
    {
        "label": "BATCH_SIZE",
        "kind": 5,
        "importPath": "neural_network.nn",
        "description": "neural_network.nn",
        "peekOfCode": "BATCH_SIZE = 64\nALPHA=0.1\n# === Stopping Criteria ===\nSTOP_CRITERIA = \"M\"\nM_SIZE = 5000            \nJ_SIZE=0.1\n# === Hyper Parameter ===\n# LAMBDA_REG=[0.000001]\n# HIDDEN_LAYER=[[64]]\nLAMBDA_REG=[0.1, 0.001, 0.000001]",
        "detail": "neural_network.nn",
        "documentation": {}
    },
    {
        "label": "STOP_CRITERIA",
        "kind": 5,
        "importPath": "neural_network.nn",
        "description": "neural_network.nn",
        "peekOfCode": "STOP_CRITERIA = \"M\"\nM_SIZE = 5000            \nJ_SIZE=0.1\n# === Hyper Parameter ===\n# LAMBDA_REG=[0.000001]\n# HIDDEN_LAYER=[[64]]\nLAMBDA_REG=[0.1, 0.001, 0.000001]\nHIDDEN_LAYER=[[64,32,16,8,4],[64,32,16,8],[64,32,16],[64,32],[64],[32]]\n# parkinsons\n# LAMBDA_REG=[5, 1, 0.5, 0.1]",
        "detail": "neural_network.nn",
        "documentation": {}
    },
    {
        "label": "M_SIZE",
        "kind": 5,
        "importPath": "neural_network.nn",
        "description": "neural_network.nn",
        "peekOfCode": "M_SIZE = 5000            \nJ_SIZE=0.1\n# === Hyper Parameter ===\n# LAMBDA_REG=[0.000001]\n# HIDDEN_LAYER=[[64]]\nLAMBDA_REG=[0.1, 0.001, 0.000001]\nHIDDEN_LAYER=[[64,32,16,8,4],[64,32,16,8],[64,32,16],[64,32],[64],[32]]\n# parkinsons\n# LAMBDA_REG=[5, 1, 0.5, 0.1]\n# HIDDEN_LAYER=[[22, 64, 64, 32, 1],[22, 64, 32, 1],[22, 32, 1]]",
        "detail": "neural_network.nn",
        "documentation": {}
    },
    {
        "label": "sigmoid",
        "kind": 2,
        "importPath": "neural_network.propagation",
        "description": "neural_network.propagation",
        "peekOfCode": "def sigmoid(z):\n    return 1 / (1 + np.exp(-z))\ndef sigmoid_gradient(z):\n    return sigmoid(z) * (1 - sigmoid(z))\ndef add_bias(X):\n    return np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)\ndef forward_propagation(Theta, X):\n    Theta = [np.array(t) for t in Theta]\n    A = [add_bias(np.array(X))] \n    Z = []",
        "detail": "neural_network.propagation",
        "documentation": {}
    },
    {
        "label": "sigmoid_gradient",
        "kind": 2,
        "importPath": "neural_network.propagation",
        "description": "neural_network.propagation",
        "peekOfCode": "def sigmoid_gradient(z):\n    return sigmoid(z) * (1 - sigmoid(z))\ndef add_bias(X):\n    return np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)\ndef forward_propagation(Theta, X):\n    Theta = [np.array(t) for t in Theta]\n    A = [add_bias(np.array(X))] \n    Z = []\n    for i, Theta_i in enumerate(Theta):\n        Z_i = A[-1] @ Theta_i.T",
        "detail": "neural_network.propagation",
        "documentation": {}
    },
    {
        "label": "add_bias",
        "kind": 2,
        "importPath": "neural_network.propagation",
        "description": "neural_network.propagation",
        "peekOfCode": "def add_bias(X):\n    return np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)\ndef forward_propagation(Theta, X):\n    Theta = [np.array(t) for t in Theta]\n    A = [add_bias(np.array(X))] \n    Z = []\n    for i, Theta_i in enumerate(Theta):\n        Z_i = A[-1] @ Theta_i.T\n        A_i = sigmoid(Z_i)\n        Z.append(Z_i)",
        "detail": "neural_network.propagation",
        "documentation": {}
    },
    {
        "label": "forward_propagation",
        "kind": 2,
        "importPath": "neural_network.propagation",
        "description": "neural_network.propagation",
        "peekOfCode": "def forward_propagation(Theta, X):\n    Theta = [np.array(t) for t in Theta]\n    A = [add_bias(np.array(X))] \n    Z = []\n    for i, Theta_i in enumerate(Theta):\n        Z_i = A[-1] @ Theta_i.T\n        A_i = sigmoid(Z_i)\n        Z.append(Z_i)\n        if i < len(Theta) - 1:\n            A_i = add_bias(A_i) # add bias in hidden layer only",
        "detail": "neural_network.propagation",
        "documentation": {}
    },
    {
        "label": "cost_function",
        "kind": 2,
        "importPath": "neural_network.propagation",
        "description": "neural_network.propagation",
        "peekOfCode": "def cost_function(A_final, Y, Theta, lambda_reg):\n    m = Y.shape[0]\n    cost = -np.sum(Y * np.log(A_final) + (1 - Y) * np.log(1 - A_final)) / m\n    reg_term = 0\n    for theta in Theta:\n        theta = np.array(theta)\n        # remove bias -> [:, 1:]\n        reg_term += np.sum(theta[:, 1:] ** 2) \n    reg_term = reg_term * (lambda_reg / (2 * m))\n    return cost, cost + reg_term",
        "detail": "neural_network.propagation",
        "documentation": {}
    },
    {
        "label": "backpropagation_vectorized",
        "kind": 2,
        "importPath": "neural_network.propagation",
        "description": "neural_network.propagation",
        "peekOfCode": "def backpropagation_vectorized(Theta, A, Z, Y, lambda_reg):\n    Theta = [np.array(t) for t in Theta]\n    m = Y.shape[0]\n    delta = A[-1] - Y\n    gradients = [None] * len(Theta)\n    for i in reversed(range(len(Theta))):\n        a_prev = A[i]\n        gradients[i] = (delta.T @ a_prev) / m\n        if i > 0:\n            delta = (delta @ Theta[i][:, 1:]) * sigmoid_gradient(Z[i - 1])",
        "detail": "neural_network.propagation",
        "documentation": {}
    },
    {
        "label": "run_debug",
        "kind": 2,
        "importPath": "neural_network.propagation",
        "description": "neural_network.propagation",
        "peekOfCode": "def run_debug(Theta, X, y, lambda_reg, DEBUG_FILENAME):\n    np.set_printoptions(precision=5, suppress=True, floatmode='fixed')\n    A, Z, all_a_lists, all_z_lists = forward_propagation(Theta, X)\n    pred_y_list = [a_list[-1] for a_list in all_a_lists]\n    true_y_list = [y[i].reshape(-1, 1) for i in range(y.shape[0])]\n    J_list = []\n    for pred, true in zip(pred_y_list, true_y_list):\n        J = -(true.T @ np.log(pred) + (1 - true).T @ np.log(1 - pred))\n        J_list.append(J.item())\n    delta_list = []",
        "detail": "neural_network.propagation",
        "documentation": {}
    },
    {
        "label": "Node",
        "kind": 6,
        "importPath": "random_forest.tree",
        "description": "random_forest.tree",
        "peekOfCode": "class Node:\n    def __init__(self, feature=None, threshold=None, label=None, children=None):\n        self.feature = feature\n        self.threshold = threshold\n        self.label = label\n        self.children = children if children else {}\ndef entropy(y):\n    probabilities = y.value_counts() / len(y)\n    return -np.sum(probabilities * np.log2(probabilities))\ndef build_tree(X, y, features, depth=0):",
        "detail": "random_forest.tree",
        "documentation": {}
    },
    {
        "label": "load_and_preprocess_data",
        "kind": 2,
        "importPath": "random_forest.tree",
        "description": "random_forest.tree",
        "peekOfCode": "def load_and_preprocess_data(DATASET_NAME):\n    data = pd.read_csv(f\"../datasets/{DATASET_NAME}.csv\")\n    data = data.rename(columns={\"class\": \"label\"})\n    if DATASET_NAME == \"parkinsons\":\n        data.rename(columns={\"Diagnosis\": \"label\"}, inplace=True)\n    for col in data.columns:\n        if col == \"label\":\n            continue\n        elif col.endswith(\"_cat\"):\n            data[col] = data[col].astype(str)",
        "detail": "random_forest.tree",
        "documentation": {}
    },
    {
        "label": "cross_validation",
        "kind": 2,
        "importPath": "random_forest.tree",
        "description": "random_forest.tree",
        "peekOfCode": "def cross_validation(data, k_fold):\n    class_0 = data[data['label'] == 0].sample(frac=1).reset_index(drop=True)\n    class_1 = data[data['label'] == 1].sample(frac=1).reset_index(drop=True)\n    all_data = pd.DataFrame()\n    for i in range(k_fold):\n        class_0_fold = class_0.iloc[int(len(class_0)*i/k_fold):int(len(class_0)*(i+1)/k_fold)]\n        class_1_fold = class_1.iloc[int(len(class_1)*i/k_fold):int(len(class_1)*(i+1)/k_fold)]\n        fold_data = pd.concat([class_0_fold, class_1_fold]).copy()\n        fold_data[\"k_fold\"] = i\n        all_data = pd.concat([all_data, fold_data], ignore_index=True)",
        "detail": "random_forest.tree",
        "documentation": {}
    },
    {
        "label": "bootstrap_sample",
        "kind": 2,
        "importPath": "random_forest.tree",
        "description": "random_forest.tree",
        "peekOfCode": "def bootstrap_sample(X, y):\n    idxs = np.random.choice(len(X), size=len(X), replace=True)\n    return X.iloc[idxs].reset_index(drop=True), y.iloc[idxs].reset_index(drop=True)\ndef main(DATASET_NAME):\n    os.makedirs(\"plot\", exist_ok=True)\n    os.makedirs(\"table\", exist_ok=True)\n    data = load_and_preprocess_data(DATASET_NAME)\n    fold_data = cross_validation(data, k_fold=K_FOLD_SIZE)\n    ntrees_list, metrics, predict_y = evaluate_random_forest(fold_data, K_FOLD_SIZE, DATASET_NAME)\n    plot_metrics(ntrees_list, metrics, DATASET_NAME)",
        "detail": "random_forest.tree",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "random_forest.tree",
        "description": "random_forest.tree",
        "peekOfCode": "def main(DATASET_NAME):\n    os.makedirs(\"plot\", exist_ok=True)\n    os.makedirs(\"table\", exist_ok=True)\n    data = load_and_preprocess_data(DATASET_NAME)\n    fold_data = cross_validation(data, k_fold=K_FOLD_SIZE)\n    ntrees_list, metrics, predict_y = evaluate_random_forest(fold_data, K_FOLD_SIZE, DATASET_NAME)\n    plot_metrics(ntrees_list, metrics, DATASET_NAME)\n    return predict_y\ndef evaluate_random_forest(fold_data, k_fold, DATASET_NAME):\n    ntrees_list = [1, 5, 10, 20, 30, 40, 50]",
        "detail": "random_forest.tree",
        "documentation": {}
    },
    {
        "label": "evaluate_random_forest",
        "kind": 2,
        "importPath": "random_forest.tree",
        "description": "random_forest.tree",
        "peekOfCode": "def evaluate_random_forest(fold_data, k_fold, DATASET_NAME):\n    ntrees_list = [1, 5, 10, 20, 30, 40, 50]\n    acc_list, prec_list, rec_list, f1_list = [], [], [], []\n    final_predictions = None\n    for ntrees in ntrees_list:\n        print(f\"\\nEvaluating Random Forest with {ntrees} trees\")\n        accs, precisions, recalls, f1s = [], [], [], []\n        for i in range(k_fold):\n            test_data = fold_data[fold_data[\"k_fold\"] == i]\n            train_data = fold_data[fold_data[\"k_fold\"] != i]",
        "detail": "random_forest.tree",
        "documentation": {}
    },
    {
        "label": "entropy",
        "kind": 2,
        "importPath": "random_forest.tree",
        "description": "random_forest.tree",
        "peekOfCode": "def entropy(y):\n    probabilities = y.value_counts() / len(y)\n    return -np.sum(probabilities * np.log2(probabilities))\ndef build_tree(X, y, features, depth=0):\n    if len(y.unique()) == 1 or len(features) == 0 or depth == MAX_DEPTH:\n        return Node(label=y.mode()[0])\n    m = int(math.sqrt(len(features)))\n    selected_features = random.sample(list(features), m)\n    best_feature, best_gain, best_threshold = None, -1, None\n    for feature in selected_features:",
        "detail": "random_forest.tree",
        "documentation": {}
    },
    {
        "label": "build_tree",
        "kind": 2,
        "importPath": "random_forest.tree",
        "description": "random_forest.tree",
        "peekOfCode": "def build_tree(X, y, features, depth=0):\n    if len(y.unique()) == 1 or len(features) == 0 or depth == MAX_DEPTH:\n        return Node(label=y.mode()[0])\n    m = int(math.sqrt(len(features)))\n    selected_features = random.sample(list(features), m)\n    best_feature, best_gain, best_threshold = None, -1, None\n    for feature in selected_features:\n        if feature.endswith('_cat'):\n            values = X[feature].unique()\n            weighted_entropy = sum((len(y[X[feature] == v]) / len(y)) * entropy(y[X[feature] == v]) for v in values)",
        "detail": "random_forest.tree",
        "documentation": {}
    },
    {
        "label": "random_forest_predict",
        "kind": 2,
        "importPath": "random_forest.tree",
        "description": "random_forest.tree",
        "peekOfCode": "def random_forest_predict(trees, X_test):\n    tree_preds = np.array([predict(tree, X_test) for tree in trees])\n    final_preds = []\n    for i in range(X_test.shape[0]):\n        row_preds = tree_preds[:, i]\n        values, counts = np.unique(row_preds[row_preds != None], return_counts=True)\n        if len(counts) == 0:\n            final_preds.append(None)\n        elif len(counts) > 1 and counts[0] == counts[1]:\n            final_preds.append(random.choice(values))",
        "detail": "random_forest.tree",
        "documentation": {}
    },
    {
        "label": "predict",
        "kind": 2,
        "importPath": "random_forest.tree",
        "description": "random_forest.tree",
        "peekOfCode": "def predict(tree, X_test):\n    predictions = []\n    for _, row in X_test.iterrows():\n        node = tree\n        while node.label is None:\n            val = row[node.feature]\n            if node.threshold is None:\n                node = node.children.get(val)\n            else:\n                node = node.children.get(\"<=\") if val <= node.threshold else node.children.get(\">\")",
        "detail": "random_forest.tree",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "kind": 2,
        "importPath": "random_forest.tree",
        "description": "random_forest.tree",
        "peekOfCode": "def accuracy(predictions, true_labels):\n    predictions, true_labels = np.array(predictions), np.array(true_labels)\n    valid = predictions != None\n    return np.mean(predictions[valid] == true_labels[valid])\ndef precision(y_true, y_pred):\n    tp = np.sum((y_pred == 1) & (y_true == 1))\n    fp = np.sum((y_pred == 1) & (y_true == 0))\n    return tp / (tp + fp) if (tp + fp) > 0 else 0.0\ndef recall(y_true, y_pred):\n    tp = np.sum((y_pred == 1) & (y_true == 1))",
        "detail": "random_forest.tree",
        "documentation": {}
    },
    {
        "label": "precision",
        "kind": 2,
        "importPath": "random_forest.tree",
        "description": "random_forest.tree",
        "peekOfCode": "def precision(y_true, y_pred):\n    tp = np.sum((y_pred == 1) & (y_true == 1))\n    fp = np.sum((y_pred == 1) & (y_true == 0))\n    return tp / (tp + fp) if (tp + fp) > 0 else 0.0\ndef recall(y_true, y_pred):\n    tp = np.sum((y_pred == 1) & (y_true == 1))\n    fn = np.sum((y_pred == 0) & (y_true == 1))\n    return tp / (tp + fn) if (tp + fn) > 0 else 0.0\ndef f1_score_manual(y_true, y_pred):\n    p = precision(y_true, y_pred)",
        "detail": "random_forest.tree",
        "documentation": {}
    },
    {
        "label": "recall",
        "kind": 2,
        "importPath": "random_forest.tree",
        "description": "random_forest.tree",
        "peekOfCode": "def recall(y_true, y_pred):\n    tp = np.sum((y_pred == 1) & (y_true == 1))\n    fn = np.sum((y_pred == 0) & (y_true == 1))\n    return tp / (tp + fn) if (tp + fn) > 0 else 0.0\ndef f1_score_manual(y_true, y_pred):\n    p = precision(y_true, y_pred)\n    r = recall(y_true, y_pred)\n    return 2 * p * r / (p + r) if (p + r) > 0 else 0.0\ndef plot_metrics(ntrees_list, metrics, DATASET_NAME):\n    titles = [\"Accuracy\", \"Precision\", \"Recall\", \"F1Score\"]",
        "detail": "random_forest.tree",
        "documentation": {}
    },
    {
        "label": "f1_score_manual",
        "kind": 2,
        "importPath": "random_forest.tree",
        "description": "random_forest.tree",
        "peekOfCode": "def f1_score_manual(y_true, y_pred):\n    p = precision(y_true, y_pred)\n    r = recall(y_true, y_pred)\n    return 2 * p * r / (p + r) if (p + r) > 0 else 0.0\ndef plot_metrics(ntrees_list, metrics, DATASET_NAME):\n    titles = [\"Accuracy\", \"Precision\", \"Recall\", \"F1Score\"]\n    for metric, title in zip(metrics, titles):\n        plt.figure(figsize=(6, 4))\n        plt.plot(ntrees_list, metric, marker='o')\n        plt.title(f\"{DATASET_NAME.capitalize()} Dataset_{title} vs ntrees\")",
        "detail": "random_forest.tree",
        "documentation": {}
    },
    {
        "label": "plot_metrics",
        "kind": 2,
        "importPath": "random_forest.tree",
        "description": "random_forest.tree",
        "peekOfCode": "def plot_metrics(ntrees_list, metrics, DATASET_NAME):\n    titles = [\"Accuracy\", \"Precision\", \"Recall\", \"F1Score\"]\n    for metric, title in zip(metrics, titles):\n        plt.figure(figsize=(6, 4))\n        plt.plot(ntrees_list, metric, marker='o')\n        plt.title(f\"{DATASET_NAME.capitalize()} Dataset_{title} vs ntrees\")\n        plt.xlabel(\"ntrees\")\n        plt.ylabel(title)\n        plt.grid(True)\n        plt.savefig(f\"plot/{DATASET_NAME}_{title}.png\")",
        "detail": "random_forest.tree",
        "documentation": {}
    },
    {
        "label": "tree_to_dict",
        "kind": 2,
        "importPath": "random_forest.tree",
        "description": "random_forest.tree",
        "peekOfCode": "def tree_to_dict(node):\n    if node.label is not None:\n        return {\"label\": int(node.label)}\n    return {\n        \"feature\": node.feature,\n        \"threshold\": node.threshold,\n        \"children\": {str(k): tree_to_dict(v) for k, v in node.children.items()}\n    }\ndef save_trees_as_json(trees, ntrees, base_dir=\"saved_trees\"):\n    folder = os.path.join(base_dir, f\"ntrees_{ntrees}\")",
        "detail": "random_forest.tree",
        "documentation": {}
    },
    {
        "label": "save_trees_as_json",
        "kind": 2,
        "importPath": "random_forest.tree",
        "description": "random_forest.tree",
        "peekOfCode": "def save_trees_as_json(trees, ntrees, base_dir=\"saved_trees\"):\n    folder = os.path.join(base_dir, f\"ntrees_{ntrees}\")\n    os.makedirs(folder, exist_ok=True)\n    for i, tree in enumerate(trees, start=1):\n        tree_dict = tree_to_dict(tree)\n        with open(os.path.join(folder, f\"tree_{i}.json\"), \"w\") as f:\n            json.dump(tree_dict, f, indent=4)\nif __name__ == \"__main__\":\n    main(DATASET_NAME=DATASET_NAME)",
        "detail": "random_forest.tree",
        "documentation": {}
    },
    {
        "label": "MAX_DEPTH",
        "kind": 5,
        "importPath": "random_forest.tree",
        "description": "random_forest.tree",
        "peekOfCode": "MAX_DEPTH = 5\nMIN_INFO_GAIN = 1e-5\n# ===== Preprocessing =====\ndef load_and_preprocess_data(DATASET_NAME):\n    data = pd.read_csv(f\"../datasets/{DATASET_NAME}.csv\")\n    data = data.rename(columns={\"class\": \"label\"})\n    if DATASET_NAME == \"parkinsons\":\n        data.rename(columns={\"Diagnosis\": \"label\"}, inplace=True)\n    for col in data.columns:\n        if col == \"label\":",
        "detail": "random_forest.tree",
        "documentation": {}
    },
    {
        "label": "MIN_INFO_GAIN",
        "kind": 5,
        "importPath": "random_forest.tree",
        "description": "random_forest.tree",
        "peekOfCode": "MIN_INFO_GAIN = 1e-5\n# ===== Preprocessing =====\ndef load_and_preprocess_data(DATASET_NAME):\n    data = pd.read_csv(f\"../datasets/{DATASET_NAME}.csv\")\n    data = data.rename(columns={\"class\": \"label\"})\n    if DATASET_NAME == \"parkinsons\":\n        data.rename(columns={\"Diagnosis\": \"label\"}, inplace=True)\n    for col in data.columns:\n        if col == \"label\":\n            continue",
        "detail": "random_forest.tree",
        "documentation": {}
    }
]